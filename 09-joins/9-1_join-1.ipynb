{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d78f409-10b1-47c6-b921-e26b7d5142ab",
   "metadata": {},
   "source": [
    "<link rel='stylesheet' href='../assets/css/main.css'/>\n",
    "\n",
    "# Join 1\n",
    "\n",
    "## Overview\n",
    "\n",
    "Straight up join\n",
    "\n",
    "## Duration\n",
    "\n",
    "20 mins\n",
    "\n",
    "## Step-1: Verify datsets\n",
    "\n",
    "We have 2 datasets\n",
    "\n",
    "- transactions data (large data).  Sample data is in `data/transactions/transactions-sample.csv`\n",
    "- rewards data (small data).  Sample data in `data/reward-points/reward-points.csv`\n",
    "\n",
    "Both datasets have `merchant_id` field in common.\n",
    "\n",
    "Also optionally, verify you have this data in HDFS.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d87b43f-124e-4e06-9095-44a1dcf5263f",
   "metadata": {},
   "source": [
    "## Step-2: Start up Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9815b43e-0787-45e5-8846-aad9fd88155c",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    spark\n",
    "except NameError:\n",
    "    import findspark\n",
    "    findspark.init()  # uses SPARK_HOME\n",
    "    print(\"Spark found in : \", findspark.find())\n",
    "\n",
    "    import pyspark\n",
    "    from pyspark import SparkConf\n",
    "    from pyspark.sql import SparkSession\n",
    "\n",
    "    # use a unique tmep dir for warehouse dir, so we can run multiple spark sessions in one dir\n",
    "    import tempfile\n",
    "    tmpdir = tempfile.TemporaryDirectory()\n",
    "\n",
    "    config = ( SparkConf()\n",
    "             .setAppName(\"TestApp\")\n",
    "             .setMaster(\"local[*]\")\n",
    "             .set('executor.memory', '2g')\n",
    "             .set('spark.sql.warehouse.dir', tmpdir.name)\n",
    "             .set(\"some_property\", \"some_value\") # another example\n",
    "             )\n",
    "\n",
    "    spark = SparkSession.builder.config(conf=config).getOrCreate()\n",
    "    sc = spark.sparkContext\n",
    "\n",
    "print('Spark UI running on port ' + spark.sparkContext.uiWebUrl.split(':')[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb0fa9f-76af-4b5a-8c7f-5e2deda13087",
   "metadata": {},
   "source": [
    "## Step-3: Load both datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740a1dfa-e5c8-403c-a1ca-86c594b8dee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "t1 = time.perf_counter()\n",
    "transactions = spark.read.csv(\"../data/transactions/csv\", header=True)\n",
    "t2 = time.perf_counter()  \n",
    "rewards = spark.read.csv(\"../data/reward-points/reward-points.csv\", header=True)\n",
    "t3 = time.perf_counter()\n",
    "\n",
    "print (\"Loaded transactions data in {:,.2f} ms \".format( (t2-t1)*1000))\n",
    "print (\"Loaded rewward points data in {:,.2f} ms \".format( (t3-t2)*1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d48843-ea5a-48eb-945b-437fc3f131b5",
   "metadata": {},
   "source": [
    "## Step-4: Register Tables\n",
    "\n",
    "Let's register the tables, so we can use spark sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1356b7-ea3d-4471-93be-64006402a2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions.createOrReplaceTempView(\"transactions\")\n",
    "rewards.createOrReplaceTempView(\"rewards\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d0bcc4-ecc9-4a8d-8b95-6cba9e3eab93",
   "metadata": {},
   "source": [
    "## Step-5: Quick Check on data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336864d5-c81a-4a6b-94db-6fe5b98b7e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from transactions\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc113cae-df4a-418f-9025-cba3ab61c917",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from rewards\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0c607e-ddeb-4931-bab4-40ad5d678a98",
   "metadata": {},
   "source": [
    "## Step-6: Join\n",
    "\n",
    "Join both datasets using `merchant_id` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a751d9dc-fb1d-41b7-a3dc-85f772c26e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"\"\"\n",
    "SELECT transactions.*, rewards.*  \n",
    "from transactions join rewards \n",
    "ON (transactions.merchant_id= rewards.merchant_id)\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(s).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e4789f-37f2-4673-83fb-ec19d78c27f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want a better display, convert to pandas df\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "spark.sql(s).toPandas().head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40867f7-2e96-4a16-9878-c7c3887c32da",
   "metadata": {},
   "source": [
    "## Step-7: Join Aggregate Query\n",
    "\n",
    "We will expand on the previous join query, and do an aggregate.\n",
    "\n",
    "Find the merchants with rewards.  Adjust the following sql query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c2798b-649a-4b5c-8c84-c33cde70db0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO : fix the SQL if needed\n",
    "\n",
    "import time\n",
    "\n",
    "t1 = time.perf_counter()\n",
    "\n",
    "s = \"\"\"\n",
    "SELECT transactions.merchant_id, count(*) as total_rewards\n",
    "from transactions join rewards \n",
    "ON (transactions.merchant_id= rewards.merchant_id)\n",
    "group by transactions.merchant_id\n",
    "order by total_rewards desc\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(s).show()\n",
    "\n",
    "t2 = time.perf_counter()\n",
    "\n",
    "print (\"Join in {:,.2f} ms \".format( (t2-t1)*1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e16f49-560d-4c9e-a33f-a33d2e33295f",
   "metadata": {},
   "source": [
    "## Step-8: See the query plan\n",
    "\n",
    "Use `explain` keyword.\n",
    "\n",
    "Can you spot any optimizations?\n",
    "\n",
    "Hint : Look at the physical plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af127cc4-f6df-4c71-b9d2-b49faf26f621",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined = spark.sql(s)\n",
    "joined.explain(extended=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6999d0-9a41-4751-8a1a-47712f5cc452",
   "metadata": {},
   "source": [
    "## Step-9: See the DAG on Spark UI\n",
    "\n",
    "Go to Spark UI and observe the DAG."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a7b92f-1135-4646-b370-d42041e5663b",
   "metadata": {},
   "source": [
    "## Step-10: Now Run this on Hadoop cluster\n",
    "\n",
    "Launch spark on Hadoop cluster, and load both datasets and do a join.\n",
    "\n",
    "Here is some sample code to get you started.  Adjust TODO items as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c207d9d-11dc-4faa-ad01-c199d6d4736b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO Adjust data paths accordingly\n",
    "transactions = spark.read.csv(\"/user/me/transactions/csv\", header=True)\n",
    "rewards = spark.read.csv(\"/user/me/reward-points/reward-points.csv\", header=True)\n",
    "\n",
    "transactions.createOrReplaceTempView(\"transactions\")\n",
    "rewards.createOrReplaceTempView(\"rewards\")\n",
    "\n",
    "s = \"\"\"\n",
    "SELECT transactions.merchant_id, count(*) as total_rewards\n",
    "from transactions join rewards \n",
    "ON (transactions.merchant_id= rewards.merchant_id)\n",
    "group by transactions.merchant_id\n",
    "order by total_rewards desc\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(s).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
