{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d78f409-10b1-47c6-b921-e26b7d5142ab",
   "metadata": {},
   "source": [
    "<link rel='stylesheet' href='../assets/css/main.css'/>\n",
    "\n",
    "# Broadcast Join \n",
    "\n",
    "## Overview\n",
    "\n",
    "Here we are joining large data and small data.  we will perform a **broadcast join**\n",
    "\n",
    "## Duration\n",
    "\n",
    "30 mins\n",
    "\n",
    "## Depends on\n",
    "\n",
    "[Lab 9.1](9-1_join-1.ipynb)\n",
    "\n",
    "## Step-1: Verify datsets\n",
    "\n",
    "We have 2 datasets\n",
    "\n",
    "- transactions data (large data).  Sample data is in `data/transactions/transactions-sample.csv`\n",
    "- rewards data (small data).  Sample data in `data/reward-points/reward-points.csv`\n",
    "\n",
    "Both datasets have `merchant_id` field in common.\n",
    "\n",
    "Also optionally, verify you have this data in HDFS.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d87b43f-124e-4e06-9095-44a1dcf5263f",
   "metadata": {},
   "source": [
    "## Step-2: Start up Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9815b43e-0787-45e5-8846-aad9fd88155c",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    spark\n",
    "except NameError:\n",
    "    import findspark\n",
    "    findspark.init()  # uses SPARK_HOME\n",
    "    print(\"Spark found in : \", findspark.find())\n",
    "\n",
    "    import pyspark\n",
    "    from pyspark import SparkConf\n",
    "    from pyspark.sql import SparkSession\n",
    "\n",
    "    # use a unique tmep dir for warehouse dir, so we can run multiple spark sessions in one dir\n",
    "    import tempfile\n",
    "    tmpdir = tempfile.TemporaryDirectory()\n",
    "\n",
    "    config = ( SparkConf()\n",
    "             .setAppName(\"TestApp\")\n",
    "             .setMaster(\"local[*]\")\n",
    "             .set('executor.memory', '2g')\n",
    "             .set('spark.sql.warehouse.dir', tmpdir.name)\n",
    "             .set(\"some_property\", \"some_value\") # another example\n",
    "             )\n",
    "\n",
    "    spark = SparkSession.builder.config(conf=config).getOrCreate()\n",
    "    sc = spark.sparkContext\n",
    "\n",
    "print('Spark UI running on port ' + spark.sparkContext.uiWebUrl.split(':')[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb0fa9f-76af-4b5a-8c7f-5e2deda13087",
   "metadata": {},
   "source": [
    "## Step-3: Load both datasets And Register Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740a1dfa-e5c8-403c-a1ca-86c594b8dee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = spark.read.csv(\"../data/transactions/csv\", header=True)\n",
    "rewards = spark.read.csv(\"../data/reward-points/reward-points.csv\", header=True)\n",
    "\n",
    "transactions.createOrReplaceTempView(\"transactions\")\n",
    "rewards.createOrReplaceTempView(\"rewards\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0c607e-ddeb-4931-bab4-40ad5d678a98",
   "metadata": {},
   "source": [
    "## Step-4: Broadcast Join\n",
    "\n",
    "We will provide a hint for broadcast join.  Broadcast small table `rewards`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a751d9dc-fb1d-41b7-a3dc-85f772c26e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "t1 = time.perf_counter()\n",
    "\n",
    "s = \"\"\"\n",
    "SELECT /*+ BROADCAST (rewards) */ \n",
    "transactions.merchant_id, count(*) as total_rewards\n",
    "from transactions join rewards \n",
    "ON (transactions.merchant_id = rewards.merchant_id)\n",
    "group by transactions.merchant_id\n",
    "order by total_rewards desc\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(s).show()\n",
    "\n",
    "t2 = time.perf_counter()\n",
    "\n",
    "print (\"Join in {:,.2f} ms \".format( (t2-t1)*1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e16f49-560d-4c9e-a33f-a33d2e33295f",
   "metadata": {},
   "source": [
    "## Step-5: See the query plan\n",
    "\n",
    "Use `explain` keyword.\n",
    "\n",
    "Can you spot any optimizations?\n",
    "\n",
    "Hint : Look at the physical plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af127cc4-f6df-4c71-b9d2-b49faf26f621",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined = spark.sql(s)\n",
    "joined.explain(extended=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6999d0-9a41-4751-8a1a-47712f5cc452",
   "metadata": {},
   "source": [
    "## Step-6: See the DAG on Spark UI\n",
    "\n",
    "Go to Spark UI and observe the DAG."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a7b92f-1135-4646-b370-d42041e5663b",
   "metadata": {},
   "source": [
    "## Step-7: Now Run this on Hadoop cluster\n",
    "\n",
    "Launch spark on Hadoop cluster, and load both datasets and do a join.\n",
    "\n",
    "Here is some sample code to get you started.  Adjust TODO items as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c207d9d-11dc-4faa-ad01-c199d6d4736b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO Adjust data paths accordingly\n",
    "transactions = spark.read.csv(\"/user/me/transactions/csv\", header=True)\n",
    "rewards = spark.read.csv(\"/user/me/reward-points/reward-points.csv\", header=True)\n",
    "\n",
    "transactions.createOrReplaceTempView(\"transactions\")\n",
    "rewards.createOrReplaceTempView(\"rewards\")\n",
    "\n",
    "s = \"\"\"\n",
    "SELECT /*+ BROADCAST (rewards) */ \n",
    "transactions.merchant_id, count(*) as total_rewards\n",
    "from transactions join rewards \n",
    "ON (transactions.merchant_id = rewards.merchant_id)\n",
    "group by transactions.merchant_id\n",
    "order by total_rewards desc\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(s).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
