{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data formats (CSV vs. Parquet vs. ORC)\n",
    "\n",
    "\n",
    "### Overview\n",
    "Comparing different data formats for Dataframes.  We will evaluate JSON, Parquet and ORC format.\n",
    "\n",
    "Background reads:\n",
    "- [Spark data frames](https://spark.apache.org/docs/latest/sql-programming-guide.html)\n",
    "- JSON format \n",
    "    - [wikipedia](https://en.wikipedia.org/wiki/JSON)\n",
    "    - [json.org](http://json.org/)\n",
    "- Parquet format\n",
    "    - [Parquet project](https://parquet.apache.org/)\n",
    "    - [parquet github](https://github.com/Parquet/parquet-format)\n",
    "    - [presentation](http://www.slideshare.net/larsgeorge/parquet-data-io-philadelphia-2013)\n",
    "- ORC format\n",
    "    + [ORC project](https://orc.apache.org/)\n",
    "    + [ORC explained](http://www.semantikoz.com/blog/orc-intelligent-big-data-file-format-hadoop-hive/)\n",
    "    + [ORC performance](http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.4.3/bk_performance_tuning/content/hive_perf_best_pract_use_orc_file_format.html)\n",
    "\n",
    "### Depends On \n",
    "None\n",
    "\n",
    "### Run time\n",
    "20-30 mins\n",
    "\n",
    "\n",
    "## STEP-1: Verify Data\n",
    "\n",
    "We will use the transaction data that you have generated before.\n",
    "\n",
    "We will assume it is under `data/transactions/csv` directory.\n",
    "\n",
    "If you don't have this, you can follow the **data-generation** labs to generate some data.  \n",
    "\n",
    "If you need to generate data, you can use script `03-data-generator/datagen-tx-medium.py`\n",
    "\n",
    "### Benchmarking tips\n",
    "\n",
    "You can start with 1 million rows of data.  \n",
    "To get accurate benchmark readings, use atleast **100 million rows** of data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP-2: Benchmarking Spreadsheet\n",
    "Download and inspect **dataformats-benchmark.xlsx** (from this directory)\n",
    "\n",
    "**We will be filling out the values in this spreadsheet**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-3 - Init Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark found in :  /home/sujee/apps/spark\n",
      "Spark UI running on port 4040\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    spark\n",
    "except NameError:\n",
    "    import findspark\n",
    "    findspark.init()  # uses SPARK_HOME\n",
    "    print(\"Spark found in : \", findspark.find())\n",
    "\n",
    "    import pyspark\n",
    "    from pyspark import SparkConf\n",
    "    from pyspark.sql import SparkSession\n",
    "\n",
    "    # use a unique tmep dir for warehouse dir, so we can run multiple spark sessions in one dir\n",
    "    import tempfile\n",
    "    tmpdir = tempfile.TemporaryDirectory()\n",
    "\n",
    "    config = ( SparkConf()\n",
    "             .setAppName(\"TestApp\")\n",
    "             .setMaster(\"local[*]\")\n",
    "             .set('executor.memory', '2g')\n",
    "             .set('spark.sql.warehouse.dir', tmpdir.name)\n",
    "             .set(\"some_property\", \"some_value\") # another example\n",
    "             )\n",
    "\n",
    "    spark = SparkSession.builder.config(conf=config).getOrCreate()\n",
    "    sc = spark.sparkContext\n",
    "\n",
    "print('Spark UI running on port ' + spark.sparkContext.uiWebUrl.split(':')[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-4: Monitoring processes\n",
    "\n",
    "It is recommended that you use a process monitor, like `top` or `atop`, to keep an eye on the processes running while the data conversions are running\n",
    "\n",
    "If you are on a linux based system, \n",
    "\n",
    "- open a terminal \n",
    "- invoke `top`  or `atop`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 5: Load Transaction data (CSV)\n",
    "\n",
    "- **==> While the import is running take a look at `atop` terminal.  Which of the resources (cpu / memory / disk_ are we maxing out?**  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded CSV in 1,314.26 ms \n",
      "DataFrame[id: string, timestamp: string, mti: int, card_number: bigint, amount_customer: double, merchant_type: int, merchant_id: int, merchant_address: string, ref_id: string, amount_merchant: double, response_code: int]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "t1 = time.perf_counter()\n",
    "\n",
    "df_csv = spark.read.csv(\"../data/transactions/csv\", header=True, inferSchema=True)\n",
    "\n",
    "t2 = time.perf_counter()\n",
    "print (\"Loaded CSV in {:,.2f} ms \".format( (t2-t1)*1000))\n",
    "print(df_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 - Save Data in Other formats\n",
    "\n",
    "Our original data is in CSV.  We will save this data in json, parquet and orc formats.\n",
    "\n",
    "As we save in each format, measure the time taken, and record it in the spreadsheet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved as json in 727.32 ms \n"
     ]
    }
   ],
   "source": [
    "## JSON\n",
    "\n",
    "import time\n",
    "\n",
    "t1 = time.perf_counter()\n",
    "\n",
    "df_csv.write.json('../data/transactions/json/', \"overwrite\")\n",
    "\n",
    "t2 = time.perf_counter()\n",
    "print (\"Saved as json in {:,.2f} ms \".format( (t2-t1)*1000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved as parquet in 1,052.35 ms \n"
     ]
    }
   ],
   "source": [
    "## parquet\n",
    "\n",
    "import time\n",
    "\n",
    "t1 = time.perf_counter()\n",
    "\n",
    "df_csv.write.parquet('../data/transactions/parquet/', \"overwrite\")\n",
    "\n",
    "t2 = time.perf_counter()\n",
    "print (\"Saved as parquet in {:,.2f} ms \".format( (t2-t1)*1000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved as orc in 809.51 ms \n"
     ]
    }
   ],
   "source": [
    "## orc\n",
    "\n",
    "import time\n",
    "\n",
    "t1 = time.perf_counter()\n",
    "\n",
    "df_csv.write.orc('../data/transactions/orc/', \"overwrite\")\n",
    "\n",
    "t2 = time.perf_counter()\n",
    "print (\"Saved as orc in {:,.2f} ms \".format( (t2-t1)*1000))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-6: Read all the data formats back\n",
    "\n",
    "And as we read each format, record the time it takes to load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded CSV in 513.01 ms \n"
     ]
    }
   ],
   "source": [
    "## read CSV\n",
    "\n",
    "import time\n",
    "\n",
    "t1 = time.perf_counter()\n",
    "\n",
    "df_csv = spark.read.csv(\"../data/transactions/csv\", header=True, inferSchema=True)\n",
    "\n",
    "t2 = time.perf_counter()\n",
    "print (\"Loaded CSV in {:,.2f} ms \".format( (t2-t1)*1000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded json in 420.41 ms \n"
     ]
    }
   ],
   "source": [
    "## JSON\n",
    "\n",
    "import time\n",
    "\n",
    "t1 = time.perf_counter()\n",
    "\n",
    "df_json = spark.read.json(\"../data/transactions/json\")\n",
    "\n",
    "t2 = time.perf_counter()\n",
    "print (\"Loaded json in {:,.2f} ms \".format( (t2-t1)*1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded parquet in 64.44 ms \n"
     ]
    }
   ],
   "source": [
    "## Parquet\n",
    "\n",
    "import time\n",
    "\n",
    "t1 = time.perf_counter()\n",
    "\n",
    "df_parquet = spark.read.parquet(\"../data/transactions/parquet/\")\n",
    "\n",
    "t2 = time.perf_counter()\n",
    "print (\"Loaded parquet in {:,.2f} ms \".format( (t2-t1)*1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded orc in 114.80 ms \n"
     ]
    }
   ],
   "source": [
    "## ORC\n",
    "\n",
    "import time\n",
    "\n",
    "t1 = time.perf_counter()\n",
    "\n",
    "df_orc = spark.read.orc(\"../data/transactions/orc/\")\n",
    "\n",
    "t2 = time.perf_counter()\n",
    "print (\"Loaded orc in {:,.2f} ms \".format( (t2-t1)*1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step-7: Query\n",
    "\n",
    "- **==> Find the max value of `amount_customer`**\n",
    "- **==> Note the time it took to run the query, and record it in spreadsheet**\n",
    "- **==> While the query is running, check `atop`**\n",
    "\n",
    "The query is basically \n",
    "\n",
    "```python\n",
    "df.agg(max('amount_customer')).show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|max(amount_customer)|\n",
      "+--------------------+\n",
      "|              1009.9|\n",
      "+--------------------+\n",
      "\n",
      "MAX in csv in 266.31 ms \n"
     ]
    }
   ],
   "source": [
    "## CSV\n",
    "import time\n",
    "\n",
    "from pyspark.sql.functions import max\n",
    "\n",
    "\n",
    "t1 = time.perf_counter()\n",
    "df_csv.agg(max('amount_customer')).show()\n",
    "t2 = time.perf_counter()\n",
    "\n",
    "print (\"MAX in csv in {:,.2f} ms \".format( (t2-t1)*1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|max(amount_customer)|\n",
      "+--------------------+\n",
      "|              1009.9|\n",
      "+--------------------+\n",
      "\n",
      "MAX in json in 325.14 ms \n"
     ]
    }
   ],
   "source": [
    "## JSON\n",
    "import time\n",
    "\n",
    "from pyspark.sql.functions import max\n",
    "\n",
    "\n",
    "t1 = time.perf_counter()\n",
    "df_json.agg(max('amount_customer')).show()\n",
    "t2 = time.perf_counter()\n",
    "\n",
    "print (\"MAX in json in {:,.2f} ms \".format( (t2-t1)*1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|max(amount_customer)|\n",
      "+--------------------+\n",
      "|              1009.9|\n",
      "+--------------------+\n",
      "\n",
      "MAX in parquet in 301.98 ms \n"
     ]
    }
   ],
   "source": [
    "## Parquet\n",
    "\n",
    "import time\n",
    "\n",
    "from pyspark.sql.functions import max\n",
    "\n",
    "\n",
    "t1 = time.perf_counter()\n",
    "df_parquet.agg(max('amount_customer')).show()\n",
    "t2 = time.perf_counter()\n",
    "\n",
    "print (\"MAX in parquet in {:,.2f} ms \".format( (t2-t1)*1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|max(amount_customer)|\n",
      "+--------------------+\n",
      "|              1009.9|\n",
      "+--------------------+\n",
      "\n",
      "MAX in orc in 253.23 ms \n"
     ]
    }
   ],
   "source": [
    "## ORC\n",
    "\n",
    "import time\n",
    "\n",
    "from pyspark.sql.functions import max\n",
    "\n",
    "\n",
    "t1 = time.perf_counter()\n",
    "df_orc.agg(max('amount_customer')).show()\n",
    "t2 = time.perf_counter()\n",
    "\n",
    "print (\"MAX in orc in {:,.2f} ms \".format( (t2-t1)*1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "attributes": {
     "classes": [
      "console"
     ],
     "id": ""
    }
   },
   "source": [
    "## Step 8 : Compare Data Sizes\n",
    "\n",
    "**==> Record the byte sizes in spreadsheet**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175M\t../data/transactions/csv\n",
      "296M\t../data/transactions/json\n",
      "88M\t../data/transactions/parquet\n",
      "83M\t../data/transactions/orc\n"
     ]
    }
   ],
   "source": [
    "# for human readable format\n",
    "!  du -skh ../data/transactions/csv  \\\n",
    "    ../data/transactions/json \\\n",
    "    ../data/transactions/parquet  \\\n",
    "    ../data/transactions/orc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-9: Run this benchmaring on a Hadoop cluster\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-10: Discussion\n",
    "\n",
    "Discuss your findings with the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
