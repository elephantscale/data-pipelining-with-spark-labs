<p><link rel='stylesheet' href='../assets/css/main.css'/></p>
<h1 id="partition-size-vs.-processing">Partition Size vs. Processing</h1>
<h2 id="overview">Overview</h2>
<p>We will experiment with how partition size affects processing</p>
<h2 id="duration">Duration</h2>
<p>30 minutes</p>
<h2 id="depends-on">Depends On</h2>
<p><a href="5-2_generate-data.html">Lab 5.2</a> - to generate data with various partition sizes.</p>
<h2 id="step-1-make-sure-you-have-generated-data">Step-1: Make sure you have generated data</h2>
<p>Follow <a href="5-2_generate-data.html">Lab 5.2</a> to generate data with various partition sizes.</p>
<p>Generate the same amount of data, say 10 million rows, into 10 partitions and 50 partitions.</p>
<ul>
<li>10 million rows as 10 partitions will result each partition size to be 174 MB, total size = 1.7 GB</li>
<li>10 million rows as 50 partitions will result each partition size to be 35 MB, total size = 1.7 GB</li>
</ul>
<h2 id="step-2-lets-do-a-simple-processing-in-spark">Step-2: Let’s do a simple processing in Spark</h2>
<h3 id="local-mode">Local Mode</h3>
<p>Launch Spark Shell</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span>   pyspark <span class="at">--master</span> <span class="st">&#39;local[*]&#39;</span>  <span class="at">--executor-memory</span> 2g  <span class="at">--driver-memory</span> 2g</span></code></pre></div>
<p>Load the data and do some processing:</p>
<p>Issue this on Spark-Shell</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="co">## First read smaller partition size (about 35 MB each)</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Adjust the path accordingly</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="co"># read parquet files</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>df1 <span class="op">=</span> spark.read.parquet(<span class="st">&#39;transactions/large/parquet&#39;</span>)</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="co"># if reading CSV</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>df1 <span class="op">=</span> spark.read.csv(<span class="st">&#39;transactions/large/csv&#39;</span>, header<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a><span class="co">### measure time</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> time</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>t1 <span class="op">=</span> time.time()</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>count <span class="op">=</span> df1.count()</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>t2 <span class="op">=</span> time.time()</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (<span class="st">&#39;count : {:,} rows,  time took : {:,.2f} secs&#39;</span>.<span class="bu">format</span>(count,  (t2<span class="op">-</span>t1)))</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a><span class="co">## Then read larger partition size (about 174 MB each)</span></span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Adjust the path accordingly</span></span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>df2 <span class="op">=</span> spark.read.parquet(<span class="st">&#39;transactions/large2/parquet&#39;</span>)</span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a><span class="co">#df2 = spark.read.csv(&#39;transactions/large2/csv&#39;, header=True)</span></span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a><span class="co">### measure time</span></span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> time</span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a>t1 <span class="op">=</span> time.time()</span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a>count <span class="op">=</span> df2.count()</span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a>t2 <span class="op">=</span> time.time()</span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (<span class="st">&#39;count : {:,} rows,  time took : {:,.2f} secs&#39;</span>.<span class="bu">format</span>(count,  (t2<span class="op">-</span>t1)))</span></code></pre></div>
<p><strong>Look at the Spark UI and note the following</strong></p>
<ul>
<li>How much time did loading the data take? Was it different for both datasets?</li>
<li>measure the processing times for the count. Do you see any noticeable difference?</li>
<li>How many tasks were used for count on both instances?</li>
</ul>
<h3 id="run-on-cluster-mode">Run on Cluster mode</h3>
<p>We are going to run this experiment on the cluster.</p>
<p>Make sure you have data generated in HDFS.</p>
<p>Start pyspark</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span>   pyspark  <span class="at">--master</span> yarn  <span class="at">--executor-memory</span> 2g  <span class="at">--driver-memory</span> 2g</span></code></pre></div>
<p>Run the same code again</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="co">## First read smaller partition size (about 35 MB each)</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Adjust the path accordingly</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="co">## read parquet</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>df1 <span class="op">=</span> spark.read.parquet(<span class="st">&#39;transactions/large/parquet&#39;</span>)</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="co">## or csv</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>df1 <span class="op">=</span> spark.read.csv(<span class="st">&#39;transactions/large/csv&#39;</span>, header<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a><span class="co">### measure time</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> time</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>t1 <span class="op">=</span> time.time()</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>count <span class="op">=</span> df1.count()</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>t2 <span class="op">=</span> time.time()</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (<span class="st">&#39;count : {:,} rows,  time took : {:,.2f} secs&#39;</span>.<span class="bu">format</span>(count,  (t2<span class="op">-</span>t1)))</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a><span class="co">## Then read larger partition size (about 174 MB each)</span></span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Adjust the path accordingly</span></span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>df2 <span class="op">=</span> spark.read.parquet(<span class="st">&#39;transactions/large2/parquet&#39;</span>)</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a><span class="co">## or csv</span></span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>df2 <span class="op">=</span> spark.read.csv(<span class="st">&#39;/user/me/data/transactions/b/csv&#39;</span>, header<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a><span class="co">### measure time</span></span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> time</span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>t1 <span class="op">=</span> time.time()</span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>count <span class="op">=</span> df2.count()</span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a>t2 <span class="op">=</span> time.time()</span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (<span class="st">&#39;count : {:,} rows,  time took : {:,.2f} secs&#39;</span>.<span class="bu">format</span>(count,  (t2<span class="op">-</span>t1)))</span></code></pre></div>
<p><strong>Look at the Spark UI and note the following</strong></p>
<ul>
<li>How much time did loading the data take? Was it different for both datasets?</li>
<li>measure the processing times for the count. Do you see any noticeable difference?</li>
<li>How many tasks were used for count on both instances?</li>
</ul>
<h2 id="takeaways">Takeaways</h2>
<p>So what is the <strong>right</strong> partition size? Unfortunately there is no formula for it.</p>
<p>The optimal partition size depends on the following:</p>
<ul>
<li>The kind of data you are processing</li>
<li>And the processing you are doing</li>
</ul>
<p>We recommend you experiment a little bit various partition sizes and see what works for you.</p>
<p>A good starting point is the <strong>default block size</strong> of HDFS.</p>
