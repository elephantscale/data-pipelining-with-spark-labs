<p><link rel='stylesheet' href='../assets/css/main.css'/></p>
<h1 id="partition-size-vs.-processing">Partition Size vs. Processing</h1>
<h2 id="overview">Overview</h2>
<p>We will experiment with how partition size affects processing</p>
<h2 id="duration">Duration</h2>
<p>30 minutes</p>
<h2 id="depends-on">Depends On</h2>
<p><a href="5-2_generate-data.html">Lab 5.2</a> - to generate data with various partition sizes.</p>
<h2 id="step-1-make-sure-you-have-generated-data">Step-1: Make sure you have generated data</h2>
<p>Follow <a href="5-2_generate-data.html">Lab 5.2</a> to generate data with various partition sizes.</p>
<p>Generate the same amount of data, say 10 million rows, into 10 partitions and 50 partitions.</p>
<ul>
<li>10 million rows as 10 partitions will result each partition size to be 174 MB, total size = 1.7 GB</li>
<li>10 million rows as 50 partitions will result each partition size to be 35 MB, total size = 1.7 GB</li>
</ul>
<h2 id="step-2-lets-do-a-simple-processing-in-spark">Step-2: Let’s do a simple processing in Spark</h2>
<h3 id="local-mode">Local Mode</h3>
<p>Launch Spark Shell</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span>   pyspark <span class="at">--master</span> <span class="st">&#39;local[*]&#39;</span>  <span class="at">--executor-memory</span> 4g  <span class="at">--driver-memory</span> 4g</span></code></pre></div>
<p>Load the data and do some processing:</p>
<p>Issue this on Spark-Shell</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="co">## First read smaller partition size (about 35 MB each)</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Adjust the path accordingly</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>df1 <span class="op">=</span> spark.read.csv(<span class="st">&#39;data/transactions/a/csv&#39;</span>, header<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>df1.count()</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="co">## Then read larger partition size (about 174 MB each)</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Adjust the path accordingly</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>df2 <span class="op">=</span> spark.read.csv(<span class="st">&#39;data/transactions/b/csv&#39;</span>, header<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>df2.count()</span></code></pre></div>
<p><strong>Look at the Spark UI and note the following</strong></p>
<ul>
<li>How much time did loading the data take? Was it different for both datasets?</li>
<li>measure the processing times for the count. Do you see any noticeable difference?</li>
<li>How many tasks were used for count on both instances?</li>
</ul>
<h3 id="run-on-cluster-mode">Run on Cluster mode</h3>
<p>We are going to run this experiment on the cluster.</p>
<p>Make sure you have data generated in HDFS.</p>
<p>Start pyspark</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span>   pyspark  <span class="at">--master</span> yarn  <span class="at">--executor-memory</span> 4g  <span class="at">--driver-memory</span> 4g</span></code></pre></div>
<p>Run the same code again</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="co">## First read smaller partition size (about 35 MB each)</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Adjust the path accordingly</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>df1 <span class="op">=</span> spark.read.csv(<span class="st">&#39;/user/me/data/transactions/a/csv&#39;</span>, header<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>df1.count()</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a><span class="co">## Then read larger partition size (about 174 MB each)</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Adjust the path accordingly</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>df2 <span class="op">=</span> spark.read.csv(<span class="st">&#39;/user/me/data/transactions/b/csv&#39;</span>, header<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>df2.count()</span></code></pre></div>
<p><strong>Look at the Spark UI and note the following</strong></p>
<ul>
<li>How much time did loading the data take? Was it different for both datasets?</li>
<li>measure the processing times for the count. Do you see any noticeable difference?</li>
<li>How many tasks were used for count on both instances?</li>
</ul>
<h2 id="takeaways">Takeaways</h2>
<p>So what is the <strong>right</strong> partition size? Unfortunately there is no formula for it.</p>
<p>The optimal partition size depends on the following:</p>
<ul>
<li>The kind of data you are processing</li>
<li>And the processing you are doing</li>
</ul>
<p>We recommend you experiment a little bit various partition sizes and see what works for you.</p>
<p>A good starting point is the <strong>default block size</strong> of HDFS.</p>
