{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caching in SQL -- Part 2\n",
    "Understand Spark SQL caching\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 : Generate Some data\n",
    "\n",
    "You can use transaction data that you generated before.  Or you can generate some as follows.\n",
    "\n",
    "Inspect and edit file [03-data-generator/datagen-tx-large.scala](../03-data-generator/datagen-tx-large.scala)\n",
    "\n",
    "```bash\n",
    "$   cd project/dir\n",
    "\n",
    "$   cd 03-data-generator\n",
    "\n",
    "$   spark-shell -i datagen-tx-large.scala\n",
    "```\n",
    "\n",
    "This will generate transaction data in `data/transactions/csv` folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 : Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    spark\n",
    "except NameError:\n",
    "    import findspark\n",
    "    findspark.init()  # uses SPARK_HOME\n",
    "    print(\"Spark found in : \", findspark.find())\n",
    "\n",
    "    import pyspark\n",
    "    from pyspark import SparkConf\n",
    "    from pyspark.sql import SparkSession\n",
    "\n",
    "    # use a unique tmep dir for warehouse dir, so we can run multiple spark sessions in one dir\n",
    "    import tempfile\n",
    "    tmpdir = tempfile.TemporaryDirectory()\n",
    "\n",
    "    config = ( SparkConf()\n",
    "             .setAppName(\"TestApp\")\n",
    "             .setMaster(\"local[*]\")\n",
    "             .set('executor.memory', '2g')\n",
    "             .set('spark.sql.warehouse.dir', tmpdir.name)\n",
    "             .set(\"some_property\", \"some_value\") # another example\n",
    "             )\n",
    "\n",
    "    spark = SparkSession.builder.config(conf=config).getOrCreate()\n",
    "    sc = spark.sparkContext\n",
    "\n",
    "print('Spark UI running on port ' + spark.sparkContext.uiWebUrl.split(':')[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "t1 = time.perf_counter()\n",
    "transactions_df = spark.read.csv(\"../data/transactions/csv\", header=True)\n",
    "t2 = time.perf_counter()\n",
    "print (\"Read file in {:,.2f} ms \".format( (t2-t1)*1000))\n",
    "\n",
    "transactions_df.createOrReplaceTempView(\"transactions\")\n",
    "print (\"registered temp table transactions\")\n",
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## see table data\n",
    "spark.sql(\"select * from transactions limit 10\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 : Query without caching\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "spark.catalog.clearCache()\n",
    "\n",
    "t1 = time.perf_counter()\n",
    "sql=\"\"\"\n",
    "select card_number, SUM(amount_customer) as total from transactions\n",
    "group by card_number \n",
    "order by total desc\n",
    "limit 10\n",
    "\"\"\"\n",
    "top10_spenders = spark.sql(sql)\n",
    "top10_spenders.show()\n",
    "t2 = time.perf_counter()\n",
    "print (\"query took {:,.2f} ms \".format( (t2-t1)*1000))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 : Explain Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top10_spenders.explain()\n",
    "\n",
    "#top10_spenders.explain(extended=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 : Cache\n",
    "\n",
    "There are 3 ways to cache\n",
    "1. dataframe.cache()  : non blocking\n",
    "2. spark.sql(\"cache table TABLE_NAME\") : blocking\n",
    "3. spark.catalog.cacheTable('tableName') : non blocking\n",
    "\n",
    "Try all these options and see the performance implications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# uncache\n",
    "spark.catalog.clearCache() ## clear all tables\n",
    "# spark.catalog.uncacheTable(\"clickstream\")  # clear just one table\n",
    "\n",
    "print (\"is 'transactions' cached : \" , spark.catalog.isCached('transactions'))\n",
    "\n",
    "t1 = time.perf_counter()\n",
    "## we have different ways to cache,\n",
    "## uncomment one of the following to test\n",
    "\n",
    "###---- option 1----\n",
    "# transactions_df.cache() \n",
    "\n",
    "### ----- option 2 ----\n",
    "# spark.sql(\"cache table transactions\");\n",
    "\n",
    "### ---- option 3\n",
    "# spark.catalog.cacheTable(\"transactions\")\n",
    "\n",
    "t2 = time.perf_counter()\n",
    "print (\"caching took {:,.2f} ms \".format( (t2-t1)*1000))\n",
    "\n",
    "print (\"is 'transactions' cached : \" , spark.catalog.isCached('transactions'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 : Query after caching\n",
    "Run the following cell to measure query time after caching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Query1 after caching\n",
    "## Note the time taken\n",
    "\n",
    "import time\n",
    "\n",
    "t1 = time.perf_counter()\n",
    "sql=\"\"\"\n",
    "select card_number, SUM(amount_customer) as total from transactions\n",
    "group by card_number \n",
    "order by total desc\n",
    "limit 10\n",
    "\"\"\"\n",
    "top10_spenders = spark.sql(sql)\n",
    "top10_spenders.show()\n",
    "t2 = time.perf_counter()\n",
    "print (\"query took {:,.2f} ms \".format( (t2-t1)*1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Query1 after caching\n",
    "## Note the time taken\n",
    "\n",
    "import time\n",
    "\n",
    "t1 = time.perf_counter()\n",
    "sql=\"\"\"\n",
    "select card_number, SUM(amount_customer) as total from transactions\n",
    "group by card_number \n",
    "order by total desc\n",
    "limit 10\n",
    "\"\"\"\n",
    "top10_spenders = spark.sql(sql)\n",
    "top10_spenders.show()\n",
    "t2 = time.perf_counter()\n",
    "print (\"query took {:,.2f} ms \".format( (t2-t1)*1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Explain Query\n",
    "You will see caching in effect!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top10_spenders.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8 : Clear Cache\n",
    "Try the following ways to clear cache\n",
    "\n",
    "1. spark.sql (\"CLEAR CACHE\")  - removes all cache\n",
    "2. spark.sql (\"CLEAR CACHE tableName\") - removes one table\n",
    "3. spark.catalog.uncacheTable('tableName') - removes one cached table\n",
    "4. spark.catalog.clearCache() - clear all caches\n",
    "5. dataframe.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"clear cache\")\n",
    "# spark.catalog.clearCacheTable('table name')\n",
    "# df.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
