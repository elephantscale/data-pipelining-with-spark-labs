{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caching\n",
    "\n",
    "### Overview\n",
    "\n",
    "Understanding Spark caching\n",
    "\n",
    "### Depends On \n",
    "\n",
    "None\n",
    "\n",
    "### Run time\n",
    "\n",
    "20-30 mins\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Generate 'Large' data set\n",
    "\n",
    "Let's generate some 'big enough' data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! echo \"creating data...\"\n",
    "\n",
    "! [ ! -r ../data/twinkle/500M.data ] && cd  ../data/twinkle &&   ./create-data-files.sh\n",
    "\n",
    "! echo \"DONE\"\n",
    "\n",
    "! ls -lSrh ../data/twinkle\n",
    "# sorted by size (smallest --> largest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Init Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    spark\n",
    "except NameError:\n",
    "    import findspark\n",
    "    findspark.init()  # uses SPARK_HOME\n",
    "    print(\"Spark found in : \", findspark.find())\n",
    "\n",
    "    import pyspark\n",
    "    from pyspark import SparkConf\n",
    "    from pyspark.sql import SparkSession\n",
    "\n",
    "    # use a unique tmep dir for warehouse dir, so we can run multiple spark sessions in one dir\n",
    "    import tempfile\n",
    "    tmpdir = tempfile.TemporaryDirectory()\n",
    "\n",
    "    config = ( SparkConf()\n",
    "             .setAppName(\"TestApp\")\n",
    "             .setMaster(\"local[*]\")\n",
    "             .set('executor.memory', '2g')\n",
    "             .set('spark.sql.warehouse.dir', tmpdir.name)\n",
    "             .set(\"some_property\", \"some_value\") # another example\n",
    "             )\n",
    "\n",
    "    spark = SparkSession.builder.config(conf=config).getOrCreate()\n",
    "    sc = spark.sparkContext\n",
    "\n",
    "print('Spark UI running on port ' + spark.sparkContext.uiWebUrl.split(':')[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 3: Recording Caching times\n",
    "\n",
    "**Download and inspect the Excel worksheet : `08-caching/caching-worksheet.xlsx`**\n",
    "\n",
    "We are going to fill in the values here to understand how caching performs.\n",
    "\n",
    "It looks like this:\n",
    "<img src=\"../assets/images/caching-1.png\" style=\"border: 5px solid grey; max-width:100%;\"/>\n",
    "\n",
    "\n",
    "## STEP 4: Load Data\n",
    "\n",
    "Load a big file (e.g 500M.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = spark.read.text(\"../data/twinkle/500M.data\")\n",
    "print(\"read file \", f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=> Count the number of lines in this file**    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "print(f.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=> Observe time taken on Spark UI**  \n",
    "**=> Record the time in spread sheet**  \n",
    "**=> Run 'count' below a couple of times and observe the time**  \n",
    "**=> Can you explain the behavior of count() execution time ?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "print (f.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "print (f.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## STEP 5:  Cache\n",
    "\n",
    "**=> Cache the file using  `cache()` action.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "f.cache()\n",
    "print (\"done caching\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=> Run the `count()` again. Notice the time.   Can you explain this behavior ?  :-)** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "print (f.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=> Run count() a few more times and note the execution times.**  \n",
    "**=> Record the time in spreadsheet.**  \n",
    "**=> Do the timings make sense?** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "print (f.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "print (f.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 6:  Understanding Cache storage\n",
    "\n",
    "Go to spark shell UI @ port 4040  \n",
    "**=> Inspect 'storage' tab**  \n",
    "\n",
    "<img src=\"../assets/images/caching-2.png\" style=\"border: 5px solid grey; max-width:100%;\"/>\n",
    "\n",
    "**=> Can you see the cached data?  What is the memory size?**  \n",
    "**=> What are the implications?** \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7 : Caching RDD vs. Dataframe\n",
    "We will load the same data using RDD API and Dataframe API will compare cache performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create some random data\n",
    "print(\"creating 100M random data\")\n",
    "!dd if=/dev/urandom of=../data/100M.data  bs=1M count=100\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "#RDD\n",
    "rdd = sc.textFile(\"../data/100M.data\")\n",
    "rdd.cache()\n",
    "print(\"rdd count \" , rdd.count())  # force caching\n",
    "\n",
    "df  = spark.read.text(\"../data/100M.data\")\n",
    "df.cache()\n",
    "print(\"df count \", df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now check the 'Storage' tab in Spark Shell UI (port 4040).  \n",
    "\n",
    "**Do you see any noticeable difference?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-8: BONUS Lab : Try Caching HDFS Data\n",
    "\n",
    "Now that we have a fairly good idea of caching mechanics, Let's try caching with HDFS data.  Use the following code to get you started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start Spark in yarn mode\n",
    "\n",
    "```bash\n",
    "$  pyspark  --yarn\n",
    "```\n",
    "\n",
    "And try the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust the data path accordingly\n",
    "df1 = spark.read.csv('/user/me/transactions/csv')\n",
    "\n",
    "# time this\n",
    "df1.count()\n",
    "\n",
    "# cache\n",
    "df1.cache()\n",
    "\n",
    "# and measure time again\n",
    "df1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
